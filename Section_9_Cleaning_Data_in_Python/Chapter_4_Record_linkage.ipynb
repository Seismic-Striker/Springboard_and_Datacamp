{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31cf85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import missingno as msno\n",
    "from fuzzywuzzy import fuzz\n",
    "import recordlinkage\n",
    "restaurants = pd.read_csv('restaurants_L2.csv', index_col=[0])\n",
    "restaurants_new = pd.read_csv('restaurants_L2_dirty.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22f781a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=['rest_name','rest_addr','city','phone','cuisine_type']\n",
    "restaurants.columns = headers\n",
    "restaurants_new.columns = headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090ba56",
   "metadata": {},
   "source": [
    "# 1. Comparing strings\n",
    "Awesome work on chapter 3! Welcome to the final chapter of this course,\n",
    "\n",
    "2. In this chapter\n",
    "where we'll discover the world of record linkage. But before we get deep dive into record linkage, let's sharpen our understanding of string similarity and minimum edit distance.\n",
    "\n",
    "3. Minimum edit distance\n",
    "Minimum edit distance is a systematic way to identify how close 2 strings are. For example, let's take a look at the following two words: intention, and execution. The minimum edit distance between them is the least possible amount of steps, that could get us from the word intention to execution, with the available operations being\n",
    "\n",
    "4. Minimum edit distance\n",
    "inserting new characters, deleting them, substituting them, and transposing consecutive characters.\n",
    "\n",
    "5. Minimum edit distance\n",
    "To get from intention to execution,\n",
    "\n",
    "6. Minimum edit distance\n",
    "We first start off by deleting I from intention, and adding C between E and N. Our minimum edit distance so far is 2, since these are two operations.\n",
    "\n",
    "7. Minimum edit distance\n",
    "Then we substitute the first N with E, T with X, and N with U, leading us to execution! With the minimum edit distance being 5.\n",
    "\n",
    "8. Minimum edit distance\n",
    "The lower the edit distance, the closer two words are. For example, the two different typos of reading have a minimum edit distance of 1 between them and reading.\n",
    "\n",
    "9. Minimum edit distance algorithms\n",
    "There's a variety of algorithms based on edit distance that differ on which operations they use, how much weight attributed to each operation, which type of strings they're suited for and more, with a variety of packages to get each similarity.\n",
    "\n",
    "10. Minimum edit distance algorithms\n",
    "For this lesson, we'll be comparing strings using Levenshtein distance since it's the most general form of string matching by using the fuzzywuzzy package.\n",
    "\n",
    "11. Simple string comparison\n",
    "Fuzzywuzzy is a simple to use package to perform string comparison. We first import fuzz from fuzzywuzzy, which allow us to compare between single strings. Here we use fuzz's WRatio function to compute the similarity between reading and its typo, inputting each string as an argument. For any comparison function using fuzzywuzzy, our output is a score from 0 to 100 with 0 being not similar at all, 100 being an exact match. Do not confuse this with the minimum edit distance score earlier, where a lower minimum edit distance means a closer match.\n",
    "\n",
    "12. Partial strings and different orderings\n",
    "The WRatio function is highly robust against partial string comparison with different orderings. For example here we compare the strings Houston Rockets and Rockets, and still receive a high similarity score. The same can be said for the strings Houston Rockets vs Los Angeles Lakers and Lakers vs Rockets, where the team names are only partial and they are differently ordered.\n",
    "\n",
    "13. Comparison with arrays\n",
    "We can also compare a string with an array of strings by using the extract function from the process module from fuzzy wuzzy. Extract takes in a string, an array of strings, and the number of possible matches to return ranked from highest to lowest. It returns a list of tuples with 3 elements, the first one being the matching string being returned, the second one being its similarity score, and the third one being its index in the array.\n",
    "\n",
    "14. Collapsing categories with string similarity\n",
    "In chapter 2, we learned that collapsing data into categories is an essential aspect of working with categorical and text data, and we saw how to manually replace categories in a column of a DataFrame. But what if we had so many inconsistent categories that a manual replacement is simply not feasible? We can easily do that with string similarity!\n",
    "\n",
    "15. Collapsing categories with string matching\n",
    "Say we have DataFrame named survey containing answers from respondents from the state of New York and California asking them how likely are you to move on a scale of 0 to 5. The state field was free text and contains hundreds of typos. Remapping them manually would take a huge amount of time. Instead, we'll use string similarity. We also have a category DataFrame containing the correct categories for each state. Let's collapse the incorrect categories with string matching!\n",
    "\n",
    "16. Collapsing all of the state\n",
    "We first create a for loop iterating over each correctly typed state in the categories DataFrame. For each state, we find its matches in the state column of the survey DataFrame, returning all possible matches by setting the limit argument of extract to the length of the survey DataFrame. Then we iterate over each potential match, isolating the ones only with a similarity score higher or equal than 80 with an if statement. Then for each of those returned strings, we replace it with the correct state using the loc method.\n",
    "\n",
    "17. Record linkage\n",
    "Record linkage attempts to join data sources that have similarly fuzzy duplicate values, so that we end up with a final DataFrame with no duplicates by using string similarity. We'll cover record linkage in more detail in the next couple of lessons.\n",
    "\n",
    "18. Let's practice!\n",
    "But for now, let's clean some data using string similarity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d139a9",
   "metadata": {},
   "source": [
    "## Minimum edit distance\n",
    "In the video exercise, you saw how minimum edit distance is used to identify how similar two strings are. As a reminder, minimum edit distance is the minimum number of steps needed to reach from String A to String B, with the operations available being:\n",
    "\n",
    "Insertion of a new character.\n",
    "Deletion of an existing character.\n",
    "Substitution of an existing character.\n",
    "Transposition of two existing consecutive characters.\n",
    "\n",
    "                    What is the minimum edit distance from 'sign' to 'sing', and which operation(s) gets you there?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "2 by substituting 'g' with 'n' and 'n' with 'g'.\n",
    "press\n",
    "1\n",
    "\n",
    "1 by transposing 'g' with 'n'.\n",
    "press\n",
    "2\n",
    "\n",
    "1 by substituting 'g' with 'n'.\n",
    "press\n",
    "3\n",
    "\n",
    "2 by deleting 'g' and inserting a new 'g' at the end.# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cd509",
   "metadata": {},
   "source": [
    "# The cutoff point\n",
    "In this exercise, and throughout this chapter, you'll be working with the restaurants DataFrame which has data on various restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "This version of restaurants has been collected from many sources, where the cuisine_type column is riddled with typos, and should contain only italian, american and asian cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "Before doing so, you want to establish the cutoff point for the similarity score using the fuzzywuzzy's process.extract() function by finding the similarity score of the most distant typo of each category.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Import process from fuzzywuzzy.\n",
    "Store the unique cuisine_types into unique_types.\n",
    "Calculate the similarity of 'asian', 'american', and 'italian' to all possible cuisine_types using process.extract(), while returning all possible matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "463b9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('italian', 67), ('american', 62), ('cajun', 40), ('southwestern', 36), ('southern', 31), ('coffeebar', 26), ('steakhouses', 25)]\n",
      "[('american', 100), ('cajun', 68), ('asian', 62), ('italian', 53), ('southwestern', 41), ('southern', 38), ('coffeebar', 24), ('steakhouses', 21)]\n",
      "[('italian', 100), ('asian', 67), ('american', 40), ('cajun', 33), ('southern', 27), ('southwestern', 26), ('steakhouses', 26), ('coffeebar', 12)]\n"
     ]
    }
   ],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4abb1",
   "metadata": {},
   "source": [
    "# Remapping categories II\n",
    "In the last exercise, you determined that the distance cutoff point for remapping typos of 'american', 'asian', and 'italian' cuisine types stored in the cuisine_type column should be 80.\n",
    "\n",
    "In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by using fuzywuzzy.process's extract() function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a string with an array of strings using process.extract(), the output is a list of tuples where each is formatted like:\n",
    "\n",
    "(closest match, similarity score, index of match)\n",
    "The restaurants DataFrame is in your environment, and you have access to a categories list containing the correct cuisine types ('italian', 'asian', and 'american').\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Return all of the unique values in the cuisine_type column of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de44c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'asian' 'italian' 'coffeebar' 'southwestern' 'steakhouses'\n",
      " 'southern' 'cajun']\n",
      "[('italian', 100, 6), ('italian', 100, 10), ('italian', 100, 11), ('italian', 100, 16), ('italian', 100, 19)]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurants['cuisine_type'].unique())\n",
    "\n",
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian',restaurants['cuisine_type'], limit =len(restaurants['cuisine_type']))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])\n",
    "\n",
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0]] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "749c90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'asian' 'italian' 'coffeebar' 'southwestern' 'steakhouses'\n",
      " 'southern' 'cajun']\n"
     ]
    }
   ],
   "source": [
    "categories = ['italian','asian','american']\n",
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "      \n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e0d58",
   "metadata": {},
   "source": [
    "# 1. Generating pairs\n",
    "Great work with lesson 1 - you now have a solid understanding how to calculate string similarity.\n",
    "\n",
    "2. Motivation\n",
    "At the end of the last video exercise, we saw how record linkage attempts to join data sources with fuzzy duplicate values. For example here are two DataFrames containing NBA games and their schedules. They've both been scraped from different sites and we would want to merge them together and have one DataFrame containing all unique games.\n",
    "\n",
    "3. When joins won't work\n",
    "We see that there are duplicates values in both DataFrames with different naming marked here in red, and non duplicate values, marked here in green. Since there are games happening at the same time, no common unique identifier between the DataFrames, and the events are differently named, a regular join or merge will not work. This is where record linkage comes in.\n",
    "\n",
    "4. Record linkage\n",
    "Record linkage is the act of linking data from different sources regarding the same entity. Generally, we clean two or more DataFrames, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them. All of these steps can be achieved with the recordlinkage package, let's find how!\n",
    "\n",
    "5. Our DataFrames\n",
    "Here we have two DataFrames, census_A, and census_B, containing data on individuals throughout the states. We want to merge them while avoiding duplication using record linkage, since they are collected manually and are prone to typos, there are no consistent IDs between them.\n",
    "\n",
    "6. Generating pairs\n",
    "We first want to generate pairs between both DataFrames. Ideally, we want to generate all possible pairs between our DataFrames.\n",
    "\n",
    "7. Generating pairs\n",
    "but what if we had big DataFrames and ended up having to generate millions if not billions of pairs? It wouldn't prove scalable and could seriously hamper development time.\n",
    "\n",
    "8. Blocking\n",
    "This is where we apply what we call blocking, which creates pairs based on a matching column, which is in this case, the state column, reducing the number of possible pairs.\n",
    "\n",
    "9. Generating pairs\n",
    "To do this, we first start off by importing recordlinkage. We then use the recordlinkage dot Index function, to create an indexing object. This essentially is an object we can use to generate pairs from our DataFrames. To generate pairs blocked on state, we use the block method, inputting the state column as input. Once the indexer object has been initialized, we generate our pairs using the dot index method, which takes in the two dataframes.\n",
    "\n",
    "10. Generating pairs\n",
    "The resulting object, is a pandas multi index object containing pairs of row indices from both DataFrames, which is a fancy way to say it is an array containing possible pairs of indices that makes it much easier to subset DataFrames on.\n",
    "\n",
    "11. Comparing the DataFrames\n",
    "Since we've already generated our pairs, it's time to find potential matches. We first start by creating a comparison object using the recordlinkage dot compare function. This is similar to the indexing object we created while generating pairs, but this one is responsible for assigning different comparison procedures for pairs. Let's say there are columns for which we want exact matches between the pairs. To do that, we use the exact method. It takes in the column name in question for each DataFrame, which is in this case date_of_birth and state, and a label argument which lets us set the column name in the resulting DataFrame. Now in order to compute string similarities between pairs of rows for columns that have fuzzy values, we use the dot string method, which also takes in the column names in question, the similarity cutoff point in the threshold argument, which takes in a value between 0 and 1, which we here set to 0.85. Finally to compute the matches, we use the compute function, which takes in the possible pairs, and the two DataFrames in question. Note that you need to always have the same order of DataFrames when inserting them as arguments when generating pairs, comparing between columns, and computing comparisons.\n",
    "\n",
    "12. Finding matching pairs\n",
    "The output is a multi index DataFrame, where the first index is the row index from the first DataFrame, or census A, and the second index is a list of all row indices in census B. The columns are the columns being compared, with values being 1 for a match, and 0 for not a match.\n",
    "\n",
    "13. Finding the only pairs we want\n",
    "To find potential matches, we just filter for rows where the sum of row values is higher than a certain threshold. Which in this case higher or equal to 2. But we'll dig deeper into these matches and see how to use them to link our census DataFrames in the next lesson.\n",
    "\n",
    "14. Let's practice!\n",
    "But for now, let's generate pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f917cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2f52a",
   "metadata": {},
   "source": [
    "# Similar restaurants\n",
    "In the last exercise, you generated pairs between restaurants and restaurants_new in an effort to cleanly merge both DataFrames using record linkage.\n",
    "\n",
    "When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact matches, string similarities, and more.\n",
    "\n",
    "Now that your pairs have been generated and stored in pairs, you will find exact matches in the city and cuisine_type columns between each pair, and similar strings for each pair in the rest_name column. Both DataFrames, pandas and recordlinkage are in your environment.\n",
    "\n",
    "Instructions 2/4\n",
    "25 XP\n",
    "2\n",
    "3\n",
    "4\n",
    "Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DataFrames.\n",
    "Use the appropriate comp_cl method to find similar strings with a 0.8 similarity threshold in the rest_name column of both DataFrames.\n",
    "\n",
    "Compute the comparison of the pairs by using the .compute() method of comp_cl.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61f11516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city  cuisine_type  name\n",
      "0   0      0             1   0.0\n",
      "    1      0             1   0.0\n",
      "    7      0             1   0.0\n",
      "    12     0             1   0.0\n",
      "    13     0             1   0.0\n",
      "...      ...           ...   ...\n",
      "40  18     0             1   0.0\n",
      "281 18     0             1   0.0\n",
      "288 18     0             1   0.0\n",
      "302 18     0             1   0.0\n",
      "308 18     0             1   0.0\n",
      "\n",
      "[3784 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types - \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30aa709",
   "metadata": {},
   "source": [
    "# 1. Linking DataFrames\n",
    "Awesome work on the first 2 lessons! You've made it to the last lesson of this course!\n",
    "\n",
    "2. Record linkage\n",
    "At this point, you've generated your pairs, compared them, and scored them.\n",
    "\n",
    "3. Record linkage\n",
    "Now it's time to link your data!\n",
    "\n",
    "4. Our DataFrames\n",
    "Remember our census DataFrames from the video of the previous lesson?\n",
    "\n",
    "5. What we've already done\n",
    "We've already generated pairs between them, compared four of their columns, two for exact matches and two for string similarity alongside a 0.85 threshold, and found potential matches.\n",
    "\n",
    "6. What we're doing now\n",
    "Now it's time to link both census DataFrames.\n",
    "\n",
    "7. Our potential matches\n",
    "Let's look closely at our potential matches. It is a multi-index DataFrame, where we have two index columns, record id 1, and record id 2.\n",
    "\n",
    "8. Our potential matches\n",
    "The first index column, stores indices from census A.\n",
    "\n",
    "9. Our potential matches\n",
    "The second index column, stores all possible indices from census_B, for each row index of census_A.\n",
    "\n",
    "10. Our potential matches\n",
    "The columns of our potential matches are the columns we chose to link both DataFrames on, where the value is 1 for a match, and 0 otherwise.\n",
    "\n",
    "11. Probable matches\n",
    "The first step in linking DataFrames, is to isolate the potentially matching pairs to the ones we're pretty sure of. We saw how to do this in the previous lesson, by subsetting the rows where the row sum is above a certain number of columns, in this case 3. The output is row indices between census A and census B that are most likely duplicates. Our next step is to extract the one of the index columns, and subsetting its associated DataFrame to filter for duplicates.\n",
    "\n",
    "12. Probable matches\n",
    "Here we choose the second index column, which represents row indices of census B. We want to extract those indices, and subset census_B on them to remove duplicates with census_A before appending them together.\n",
    "\n",
    "13. Get the indices\n",
    "We can access a DataFrame's index using the index attribute. Since this is a multi index DataFrame, it returns a multi index object containing pairs of row indices from census_A and census_B respectively. We want to extract all census_B indices, so we chain it with the get_level_values method, which takes in which column index we want to extract its values. We can either input the index column's name, or its order, which is in this case 1.\n",
    "\n",
    "14. Linking DataFrames\n",
    "To find the duplicates in census B, we simply subset on all indices of census_B, with the ones found through record linkage. You can choose to examine them further for similarity with their duplicates in census_A, but if you're sure of your analysis, you can go ahead and find the non duplicates by repeating the exact same line of code, except by adding a tilde at the beginning of your subset. Now that you have your non duplicates, all you need is a simple append using the DataFrame append method of census A, and you have your linked Data!\n",
    "\n",
    "15. Linking DataFrames\n",
    "To recap, what we did was build on top of our previous work in generating pairs, comparing across columns and finding potential matches. We then isolated all possible matches, where there are matches across 3 columns or more, ensuring we tightened our search for duplicates across both DataFrames before we link them. Extracted the row indices of census_B where there are duplicates. Found rows of census_B where they are not duplicated with census_A by using the tilde symbol. And linked both DataFrames for full census results!\n",
    "\n",
    "16. Let's practice!\n",
    "Now that you know how to link DataFrames, let's put those skills to action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aab5d3",
   "metadata": {},
   "source": [
    "# Linking them together!\n",
    "In the last lesson, you've finished the bulk of the work on your effort to link restaurants and restaurants_new. You've generated the different pairs of potentially matching rows, searched for exact matches between the cuisine_type and city columns, but compared for similar strings in the rest_name column. You stored the DataFrame containing the scores in potential_matches.\n",
    "\n",
    "Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of restaurants_new that are matching across the columns mentioned above from potential_matches. Then you will subset restaurants_new on these indices, then append the non-duplicate values to restaurants. All DataFrames are in your environment, alongside pandas imported as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.\n",
    "Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the .get_level_values() method.\n",
    "Subset restaurants_new for rows that are not in matching_indices.\n",
    "Append non_dup to restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb864fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rest_name                  rest_addr               city       phone  \\\n",
      "0     american                   american           american    american   \n",
      "1     american                   american           american    american   \n",
      "2     american                   american           american    american   \n",
      "3     american                   american           american    american   \n",
      "4     american                   american           american    american   \n",
      "..         ...                        ...                ...         ...   \n",
      "77       feast        1949 westwood blvd.            west la  3104750400   \n",
      "78    mulberry        17040 ventura blvd.             encino  8189068881   \n",
      "79  matsuhissa   129 n. la cienega blvd.       beverly hills  3106599639   \n",
      "80     jiraffe      502 santa monica blvd       santa monica  3109176671   \n",
      "81    martha's  22nd street grill 25 22nd  st. hermosa beach  3103767786   \n",
      "\n",
      "   cuisine_type  \n",
      "0      american  \n",
      "1      american  \n",
      "2      american  \n",
      "3      american  \n",
      "4      american  \n",
      "..          ...  \n",
      "77      chinese  \n",
      "78        pizza  \n",
      "79        asian  \n",
      "80  californian  \n",
      "81     american  \n",
      "\n",
      "[417 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4114991",
   "metadata": {},
   "source": [
    "# 1. Congratulations!\n",
    "Fantastic job! You have now finished the course!\n",
    "\n",
    "2. What we've learned\n",
    "In this course, you learned how to diagnose dirty data, identify the side effects of dirty data, and learned how to clean data.\n",
    "\n",
    "3. What we've learned\n",
    "In chapter 1, we learned about basic data cleaning problems such as fixing incorrect data types, making sure our data sticks within range, and dropping duplicates.\n",
    "\n",
    "4. What we've learned\n",
    "In chapter 2, we learned about common problems affecting categorical and text data.\n",
    "\n",
    "5. What we've learned\n",
    "In chapter 3, we learned about more advanced data problems, such as unifying differently formatted data, cross field validation and completeness.\n",
    "\n",
    "6. What we've learned\n",
    "Finally in chapter 4, we saw how to link datasets where joins don't work, by learning about record linkage.\n",
    "\n",
    "7. More to learn on DataCamp!\n",
    "With that in mind, there is still so much more to learn on your way to become a data cleaning expert! So make sure to check out DataCamp's content library along the way, whether that means courses,\n",
    "\n",
    "8. More to learn!\n",
    "tracks,\n",
    "\n",
    "9. More to learn!\n",
    "or projects!\n",
    "\n",
    "10. Thank you!\n",
    "Finally, don't forget to apply what you learned in your daily data tasks! Thank you for taking this journey in cleaning data in Python, and see you next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bccac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
