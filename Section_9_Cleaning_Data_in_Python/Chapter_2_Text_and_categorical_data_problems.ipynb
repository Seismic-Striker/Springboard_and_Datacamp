{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a271e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f35f7b",
   "metadata": {},
   "source": [
    "# 1. Membership constraints\n",
    "Fantastic work on Chapter 1! You're now equipped to treat more complex, and specific data cleaning problems.\n",
    "\n",
    "2. In this chapter\n",
    "In this chapter, we're going to take a look at common data problems with text and categorical data, so let's get started.\n",
    "\n",
    "3. Categories and membership constraints\n",
    "In this lesson, we'll focus on categorical variables. As discussed early in chapter 1, categorical data represent variables that represent predefined finite set of categories. Examples of this range from marriage status, household income categories, loan status and others. To run machine learning models on categorical data, they are often coded as numbers. Since categorical data represent a predefined set of categories, they can't have values that go beyond these predefined categories.\n",
    "\n",
    "4. Why could we have these problems?\n",
    "We can have inconsistencies in our categorical data for a variety of reasons. This could be due to data entry issues with free text vs dropdown fields, data parsing errors and other types of errors.\n",
    "\n",
    "5. How do we treat these problems?\n",
    "There's a variety of ways we can treat these, with increasingly specific solutions for different types of inconsistencies. Most simply, we can drop the rows with incorrect categories. We can attempt remapping incorrect categories to correct ones, and more. We'll see a variety of ways of dealing with this throughout the chapter and the course, but for now we'll just focus on dropping data.\n",
    "\n",
    "6. An example\n",
    "Let's first look at an example. Here's a DataFrame named study_data containing a list of first names, birth dates, and blood types. Additionally, a DataFrame named categories, containing the correct possible categories for the blood type column has been created as well.\n",
    "\n",
    "7. An example\n",
    "Notice the inconsistency here? There's definitely no blood type named Z+. Luckily, the categories DataFrame will help us systematically spot all rows with these inconsistencies. It's always good practice to keep a log of all possible values of your categorical data, as it will make dealing with these types of inconsistencies way easier.\n",
    "\n",
    "8. A note on joins\n",
    "Now before moving on to dealing with these inconsistent values, let's have a brief reminder on joins. The two main types of joins we care about here are anti joins and inner joins. We join DataFrames on common columns between them. Anti joins, take in two DataFrames A and B, and return data from one DataFrame that is not contained in another. In this example, we are performing a left anti join of A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on. Inner joins, return only the data that is contained in both DataFrames. For example, an inner join of A and B, would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on.\n",
    "\n",
    "9. A left anti join on blood types\n",
    "In our example, an left anti join essentially returns all the data in study data with inconsistent blood types,\n",
    "\n",
    "10. An inner join on blood types\n",
    "and an inner join returns all the rows containing consistent blood types signs.\n",
    "\n",
    "11. Finding inconsistent categories\n",
    "Now let's see how to do that in Python. We first get all inconsistent categories in the blood_type column of the study_data DataFrame. We do that by creating a set out of the blood_type column which stores its unique values, and use the difference method which takes in as argument the blood_type column from the categories DataFrame. This returns all the categories in blood_type that are not in categories. We then find the inconsistent rows by finding all the rows of the blood_type columns that are equal to inconsistent categories by using the isin method, this returns a series of boolean values that are True for inconsistent rows and False for consistent ones. We then subset the study_data DataFrame based on these boolean values, and voila we have our inconsistent data.\n",
    "\n",
    "12. Dropping inconsistent categories\n",
    "To drop inconsistent rows and keep ones that are only consistent. We just use the tilde symbol while subsetting which returns everything except inconsistent rows.\n",
    "\n",
    "13. Let's practice!\n",
    "Now that we know about treating categorical data, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb1bcf5",
   "metadata": {},
   "source": [
    "# 1. Membership constraints\n",
    "Fantastic work on Chapter 1! You're now equipped to treat more complex, and specific data cleaning problems.\n",
    "\n",
    "2. In this chapter\n",
    "In this chapter, we're going to take a look at common data problems with text and categorical data, so let's get started.\n",
    "\n",
    "3. Categories and membership constraints\n",
    "In this lesson, we'll focus on categorical variables. As discussed early in chapter 1, categorical data represent variables that represent predefined finite set of categories. Examples of this range from marriage status, household income categories, loan status and others. To run machine learning models on categorical data, they are often coded as numbers. Since categorical data represent a predefined set of categories, they can't have values that go beyond these predefined categories.\n",
    "\n",
    "4. Why could we have these problems?\n",
    "We can have inconsistencies in our categorical data for a variety of reasons. This could be due to data entry issues with free text vs dropdown fields, data parsing errors and other types of errors.\n",
    "\n",
    "5. How do we treat these problems?\n",
    "There's a variety of ways we can treat these, with increasingly specific solutions for different types of inconsistencies. Most simply, we can drop the rows with incorrect categories. We can attempt remapping incorrect categories to correct ones, and more. We'll see a variety of ways of dealing with this throughout the chapter and the course, but for now we'll just focus on dropping data.\n",
    "\n",
    "6. An example\n",
    "Let's first look at an example. Here's a DataFrame named study_data containing a list of first names, birth dates, and blood types. Additionally, a DataFrame named categories, containing the correct possible categories for the blood type column has been created as well.\n",
    "\n",
    "7. An example\n",
    "Notice the inconsistency here? There's definitely no blood type named Z+. Luckily, the categories DataFrame will help us systematically spot all rows with these inconsistencies. It's always good practice to keep a log of all possible values of your categorical data, as it will make dealing with these types of inconsistencies way easier.\n",
    "\n",
    "8. A note on joins\n",
    "Now before moving on to dealing with these inconsistent values, let's have a brief reminder on joins. The two main types of joins we care about here are anti joins and inner joins. We join DataFrames on common columns between them. Anti joins, take in two DataFrames A and B, and return data from one DataFrame that is not contained in another. In this example, we are performing a left anti join of A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on. Inner joins, return only the data that is contained in both DataFrames. For example, an inner join of A and B, would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on.\n",
    "\n",
    "9. A left anti join on blood types\n",
    "In our example, an left anti join essentially returns all the data in study data with inconsistent blood types,\n",
    "\n",
    "10. An inner join on blood types\n",
    "and an inner join returns all the rows containing consistent blood types signs.\n",
    "\n",
    "11. Finding inconsistent categories\n",
    "Now let's see how to do that in Python. We first get all inconsistent categories in the blood_type column of the study_data DataFrame. We do that by creating a set out of the blood_type column which stores its unique values, and use the difference method which takes in as argument the blood_type column from the categories DataFrame. This returns all the categories in blood_type that are not in categories. We then find the inconsistent rows by finding all the rows of the blood_type columns that are equal to inconsistent categories by using the isin method, this returns a series of boolean values that are True for inconsistent rows and False for consistent ones. We then subset the study_data DataFrame based on these boolean values, and voila we have our inconsistent data.\n",
    "\n",
    "12. Dropping inconsistent categories\n",
    "To drop inconsistent rows and keep ones that are only consistent. We just use the tilde symbol while subsetting which returns everything except inconsistent rows.\n",
    "\n",
    "13. Let's practice!\n",
    "Now that we know about treating categorical data, let's practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00be75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a csv for categories in the exercises\n",
    " \n",
    "#data = {'cleanliness' : ['Clean','Average','Somewhat clean','Somewhat dirty','Dirty'],\n",
    "#        'safety' : ['Neutral','Very safe', 'Somewhat safe', 'Very unsafe','Somewhat unsafe'],\n",
    "#        'satisfaction':['Very satisfied','Neutral','Somewhat satisfied','Somewhat Unsatisfied','Very unsatisfied']\n",
    "#       }\n",
    "#categories = pd.DataFrame(data)\n",
    "#categories.to_csv('categories.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf170bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2477 entries, 0 to 2476\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Unnamed: 0     2477 non-null   int64  \n",
      " 1   id             2477 non-null   int64  \n",
      " 2   day            2477 non-null   object \n",
      " 3   airline        2477 non-null   object \n",
      " 4   destination    2477 non-null   object \n",
      " 5   dest_region    2477 non-null   object \n",
      " 6   dest_size      2477 non-null   object \n",
      " 7   boarding_area  2477 non-null   object \n",
      " 8   dept_time      2477 non-null   object \n",
      " 9   wait_min       2477 non-null   float64\n",
      " 10  cleanliness    2477 non-null   object \n",
      " 11  safety         2477 non-null   object \n",
      " 12  satisfaction   2477 non-null   object \n",
      "dtypes: float64(1), int64(2), object(10)\n",
      "memory usage: 251.7+ KB\n"
     ]
    }
   ],
   "source": [
    "airlines = pd.read_csv('airlines_final.csv')\n",
    "categories = pd.read_csv('categories.csv')\n",
    "airlines.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed99acd",
   "metadata": {},
   "source": [
    "# Finding consistency\n",
    "In this exercise and throughout this chapter, you'll be working with the airlines DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named categories was created, containing all correct possible values for the survey columns.\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise. The pandas package has been imported as pd, and the airlines and categories DataFrames are in your environment.\n",
    "\n",
    "Instructions 1/4\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.\n",
    "Print the unique values of the survey columns in airlines using the .unique() method.\n",
    "\n",
    "Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n",
    "Find rows of airlines with a cleanliness value not in categories and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e692572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     cleanliness           safety          satisfaction\n",
      "0           0           Clean          Neutral        Very satisfied\n",
      "1           1         Average        Very safe               Neutral\n",
      "2           2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
      "3           3  Somewhat dirty      Very unsafe  Somewhat Unsatisfied\n",
      "4           4           Dirty  Somewhat unsafe      Very unsatisfied\n",
      "Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
      "\n",
      "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satisfaction:  0           Very satisfied\n",
      "1           Very satisfied\n",
      "2                  Neutral\n",
      "3       Somewhat satsified\n",
      "4       Somewhat satsified\n",
      "               ...        \n",
      "2472    Somewhat satsified\n",
      "2473        Very satisfied\n",
      "2474        Very satisfied\n",
      "2475        Very satisfied\n",
      "2476    Somewhat satsified\n",
      "Name: satisfaction, Length: 2477, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a1cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     cleanliness           safety          satisfaction\n",
      "0           0           Clean          Neutral        Very satisfied\n",
      "1           1         Average        Very safe               Neutral\n",
      "2           2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
      "3           3  Somewhat dirty      Very unsafe  Somewhat Unsatisfied\n",
      "4           4           Dirty  Somewhat unsafe      Very unsatisfied\n",
      "Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
      "\n",
      "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f30b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, id, day, airline, destination, dest_region, dest_size, boarding_area, dept_time, wait_min, cleanliness, safety, satisfaction]\n",
      "Index: []\n",
      "      Unnamed: 0    id        day        airline        destination  \\\n",
      "0              0  1351    Tuesday    UNITED INTL             KANSAI   \n",
      "1              1   373     Friday         ALASKA  SAN JOSE DEL CABO   \n",
      "2              2  2820   Thursday          DELTA        LOS ANGELES   \n",
      "3              3  1157    Tuesday      SOUTHWEST        LOS ANGELES   \n",
      "4              4  2992  Wednesday       AMERICAN              MIAMI   \n",
      "...          ...   ...        ...            ...                ...   \n",
      "2472        2804  1475    Tuesday         ALASKA       NEW YORK-JFK   \n",
      "2473        2805  2222   Thursday      SOUTHWEST            PHOENIX   \n",
      "2474        2806  2684     Friday         UNITED            ORLANDO   \n",
      "2475        2807  2549    Tuesday        JETBLUE         LONG BEACH   \n",
      "2476        2808  2162   Saturday  CHINA EASTERN            QINGDAO   \n",
      "\n",
      "        dest_region dest_size boarding_area   dept_time  wait_min  \\\n",
      "0              Asia       Hub  Gates 91-102  2018-12-31     115.0   \n",
      "1     Canada/Mexico     Small   Gates 50-59  2018-12-31     135.0   \n",
      "2           West US       Hub   Gates 40-48  2018-12-31      70.0   \n",
      "3           West US       Hub   Gates 20-39  2018-12-31     190.0   \n",
      "4           East US       Hub   Gates 50-59  2018-12-31     559.0   \n",
      "...             ...       ...           ...         ...       ...   \n",
      "2472        East US       Hub   Gates 50-59  2018-12-31     280.0   \n",
      "2473        West US       Hub   Gates 20-39  2018-12-31     165.0   \n",
      "2474        East US       Hub   Gates 70-90  2018-12-31      92.0   \n",
      "2475        West US     Small    Gates 1-12  2018-12-31      95.0   \n",
      "2476           Asia     Large    Gates 1-12  2018-12-31     220.0   \n",
      "\n",
      "         cleanliness         safety        satisfaction  \n",
      "0              Clean        Neutral      Very satisfied  \n",
      "1              Clean      Very safe      Very satisfied  \n",
      "2            Average  Somewhat safe             Neutral  \n",
      "3              Clean      Very safe  Somewhat satsified  \n",
      "4     Somewhat clean      Very safe  Somewhat satsified  \n",
      "...              ...            ...                 ...  \n",
      "2472  Somewhat clean        Neutral  Somewhat satsified  \n",
      "2473           Clean      Very safe      Very satisfied  \n",
      "2474           Clean      Very safe      Very satisfied  \n",
      "2475           Clean  Somewhat safe      Very satisfied  \n",
      "2476           Clean      Very safe  Somewhat satsified  \n",
      "\n",
      "[2477 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fee03",
   "metadata": {},
   "source": [
    "# 1. Categorical variables\n",
    "Awesome work on the last lesson. Now let's discuss other types of problems that could affect categorical variables.\n",
    "\n",
    "2. What type of errors could we have?\n",
    "In the last lesson, we saw how categorical data has a value membership constraint, where columns need to have a predefined set of values. However, this is not the only set of problems we may encounter. When cleaning categorical data, some of the problems we may encounter include value inconsistency, the presence of too many categories that could be collapsed into one, and making sure data is of the right type.\n",
    "\n",
    "3. Value consistency\n",
    "Let's start with making sure our categorical data is consistent. A common categorical data problem is having values that slightly differ because of capitalization. Not treating this could lead to misleading results when we decide to analyze our data, for example, let's assume we're working with a demographics dataset, and we have a marriage status column with inconsistent capitalization. Here's what counting the number of married people in the marriage_status Series would look like. Note that the dot-value_counts() methods works on Series only.\n",
    "\n",
    "4. Value consistency\n",
    "For a DataFrame, we can groupby the column and use the dot-count() method.\n",
    "\n",
    "5. Value consistency\n",
    "To deal with this, we can either capitalize or lowercase the marriage_status column. This can be done with the str-dot-upper() or dot-lower() functions respectively.\n",
    "\n",
    "6. Value consistency\n",
    "Another common problem with categorical values are leading or trailing spaces. For example, imagine the same demographics DataFrame containing values with leading spaces. Here's what the counts of married vs unmarried people would look like. Note that there is a married category with a trailing space on the right, which makes it hard to spot on the output, as opposed to unmarried.\n",
    "\n",
    "7. Value consistency\n",
    "To remove leading spaces, we can use the str-dot-strip() method which when given no input, strips all leading and trailing white spaces.\n",
    "\n",
    "8. Collapsing data into categories\n",
    "Sometimes, we may want to create categories out of our data, such as creating household income groups from income data. To create categories out of data, let's use the example of creating an income group column in the demographics DataFrame. We can do this in 2 ways. The first method utilizes the qcut function from pandas, which automatically divides our data based on its distribution into the number of categories we set in the q argument, we created the category names in the group_names list and fed it to the labels argument, returning the following. Notice that the first row actually misrepresents the actual income of the income group, as we didn't instruct qcut where our ranges actually lie.\n",
    "\n",
    "9. Collapsing data into categories\n",
    "We can do this with the cut function instead, which lets us define category cutoff ranges with the bins argument. It takes in a list of cutoff points for each category, with the final one being infinity represented with np-dot-inf(). From the output, we can see this is much more correct.\n",
    "\n",
    "10. Collapsing data into categories\n",
    "Sometimes, we may want to reduce the amount of categories we have in our data. Let's move on to mapping categories to fewer ones. For example, assume we have a column containing the operating system of different devices, and contains these unique values. Say we want to collapse these categories into 2, DesktopOS, and MobileOS. We can do this using the replace method. It takes in a dictionary that maps each existing category to the category name you desire. In this case, this is the mapping dictionary. A quick print of the unique values of operating system shows the mapping has been complete.\n",
    "\n",
    "11. Let's practice!\n",
    "Now that we know about treating categorical data, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813787d",
   "metadata": {},
   "source": [
    "# Inconsistent categories\n",
    "In this exercise, you'll be revisiting the airlines DataFrame from the previous lesson.\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport.\n",
    "\n",
    "In this exercise, you will examine two categorical columns from this DataFrame, dest_region and dest_size respectively, assess how to address them and make sure that they are cleaned and ready for analysis. The pandas package has been imported as pd, and the airlines DataFrame is in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0809505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east']\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n",
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n",
      "['Hub' 'Small' 'Medium' 'Large']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd57473",
   "metadata": {},
   "source": [
    "# Remapping categories\n",
    "To better understand survey respondents from airlines, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The airlines DataFrame contains the day and wait_min columns, which are categorical and numerical respectively. The day column contains the exact day a flight took place, and wait_min contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "\n",
    "wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+\n",
    "day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend.\n",
    "The pandas and numpy packages have been imported as pd and np. Let's create some new categorical data!\n",
    "\n",
    "Instructions\n",
    "0 XP\n",
    "Create the ranges and labels for the wait_type column mentioned in the description above.\n",
    "Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.\n",
    "Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.\n",
    "Create the day_week column by using .replace()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "642d8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                               labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205c035c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['weekday', 'weekend'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines['day_week'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24031b47",
   "metadata": {},
   "source": [
    "# Removing titles and taking names\n",
    "While collecting survey respondent metadata in the airlines DataFrame, the full name of respondents was saved in the full_name column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as \"Dr.\", \"Mr.\", \"Ms.\" and \"Miss\".\n",
    "\n",
    "Your ultimate objective is to create two new columns named first_name and last_name, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics.\n",
    "\n",
    "The airlines DataFrame is in your environment, alongside pandas as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Remove \"Dr.\", \"Mr.\", \"Miss\" and \"Ms.\" from full_name by replacing them with an empty string \"\" in that order.\n",
    "Run the assert statement using .str.contains() that tests whether full_name still contains any of the honorifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ad1dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'full_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'full_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2e144168a2b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Replace \"Dr.\" with empty string \"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mairlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dr.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Replace \"Mr.\" with empty string \"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mairlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mr.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'full_name'"
     ]
    }
   ],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1180876",
   "metadata": {},
   "source": [
    "# Keeping it descriptive\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the survey_response column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than 40 , and make sure your new DataFrame contains responses with 40 characters or more using an assert statement.\n",
    "\n",
    "The airlines DataFrame is in your environment, and pandas is imported as pd.\n",
    "\n",
    "Instructions\n",
    "0 XP\n",
    "Using the airlines DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len().\n",
    "Isolate the rows of airlines with resp_length higher than 40.\n",
    "Assert that the smallest survey_response length in airlines_survey is now bigger than 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2ff5a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'survey_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'survey_response'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5232dca1b7d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Store length of each row in survey_response column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresp_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'survey_response'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Find rows in airlines where resp_length > 40\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mairlines_survey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresp_length\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'survey_response'"
     ]
    }
   ],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa062a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653ecaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
