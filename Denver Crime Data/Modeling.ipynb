{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce2bbfd-441f-45d6-9e79-98e3bc3c53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "sns.set_theme()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b1d4c5-f184-42ff-877a-c44870a99b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derek\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score,log_loss\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba731ee7-95f0-45ef-b6ab-19d9ca2bbf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('df1.csv',index_col=0)\n",
    "df2 = pd.read_csv('df2.csv',index_col=0)\n",
    "df3 = pd.read_csv('df3.csv',index_col=0)\n",
    "df4 = pd.read_csv('df4.csv',index_col=0)\n",
    "df5 = pd.read_csv('df5.csv',index_col=0)\n",
    "df6 = pd.read_csv('df6.csv',index_col=0)\n",
    "df7 = pd.read_csv('df7.csv',index_col=0)\n",
    "sets=[df1,df2,df3,df4,df5,df6,df7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf2e92-1653-4536-8ccf-cb672d5bd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection RandomizedSearchCV \n",
    "\n",
    "for df in sets:\n",
    "    y = df.iloc[:,0] \n",
    "    X = df.iloc[:,1:]\n",
    "    X=pd.get_dummies(X)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "#     scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "#     X_train_scaled=scaler.transform(X_train)\n",
    "#     X_test_scaled=scaler.transform(X_test)\n",
    "    param_grid = {'n_neighbors':np.arange(1,50)}\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_cv= GridSearchCV(knn,param_grid,cv=5)\n",
    "    knn_cv.fit(X,y)\n",
    "    print(\"Best Score:\" + str(knn_cv.best_score_))\n",
    "    print(\"Best Parameters: \" + str(knn_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf389c37-cf74-498c-a62c-c6fc8f13922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in sets:\n",
    "    y = df.iloc[:,0]\n",
    "    X = df.iloc[:,1:]\n",
    "    X=pd.get_dummies(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled=scaler.transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "    clf = RandomForestClassifier(n_estimators=25, random_state = 1,n_jobs=-1)\n",
    "    model_res = clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = model_res.predict(X_test_scaled)\n",
    "    y_pred_prob = model_res.predict_proba(X_test_scaled)\n",
    "    lr_probs = y_pred_prob[:,1]\n",
    "    ac = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('Random Forest: Accuracy=%.3f' % (ac))\n",
    "    print('Random Forest: f1-score=%.3f' % (f1))\n",
    "    feature_importance = clf.feature_importances_\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "    sorted_idx = np.argsort(feature_importance)[:30]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    sorted_idx.size\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a112c-65f7-4d60-af4a-e8d79212d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in sets:\n",
    "    y = df.iloc[:,0]\n",
    "    X = df.iloc[:,1:]\n",
    "    X=pd.get_dummies(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled=scaler.transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "    xg_cl = xgb.XGBClassifier(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "    xg_cl.fit(X_train_scaled,y_train)\n",
    "    preds = xg_cl.predict(X_test)\n",
    "    accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "    print(\"accuracy: %f\" % (accuracy))\n",
    "    feature_importance = xg_cl.feature_importances_\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "    sorted_idx = np.argsort(feature_importance)[:30]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    sorted_idx.size  \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea7612-0f20-47fd-874a-505e0fca3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for df in sets:\n",
    "    y = df.iloc[:,0]\n",
    "    X = df.iloc[:,1:]\n",
    "    X=pd.get_dummies(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled=scaler.transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "    for learning_rate in learning_rates:\n",
    "        gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n",
    "        gb.fit(X_train_scaled, y_train)\n",
    "        print(\"Learning rate: \", learning_rate)\n",
    "        print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_scaled, y_train)))\n",
    "        print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test_scaled, y_test)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc587e19-a691-477e-a516-f004b1ef8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for df in sets:\n",
    "    y = df.iloc[:,0]\n",
    "    X = df.iloc[:,1:]\n",
    "    X=pd.get_dummies(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled=scaler.transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "    clf = AdaBoostClassifier(n_estimators=10)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    clf.predict(y_test.reshape(-1, 1))\n",
    "    preds = clf.predict(X_test_scaled)\n",
    "    accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "    print(\"accuracy: %f\" % (accuracy))\n",
    "    # feature_importance = clf.feature_importances_\n",
    "    # # make importances relative to max importance\n",
    "    # feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "    # sorted_idx = np.argsort(feature_importance)[:30]\n",
    "    # pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    # sorted_idx.size  \n",
    "    # plt.figure(figsize=(10,10))\n",
    "    # plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    # plt.yticks(pos, X.columns[sorted_idx])\n",
    "    # plt.xlabel('Relative Importance')\n",
    "    # plt.title('Variable Importance')\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9728b09-809b-4d04-8c75-2c15ccd24c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "dont run\n",
    "for df in sets:\n",
    "    y = df.iloc[:,0] \n",
    "    X = df.iloc[:,1:]\n",
    "    X=pd.get_dummies(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_scaled=scaler.transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    clf.predict(y_test)\n",
    "    preds = clf.predict(X_test_scaled)\n",
    "    accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "    print(\"accuracy: %f\" % (accuracy))\n",
    "    # feature_importance = clf.feature_importances_\n",
    "    # # make importances relative to max importance\n",
    "    # feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "    # sorted_idx = np.argsort(feature_importance)[:30]\n",
    "    # pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    # sorted_idx.size  \n",
    "    # plt.figure(figsize=(10,10))\n",
    "    # plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    # plt.yticks(pos, X.columns[sorted_idx])\n",
    "    # plt.xlabel('Relative Importance')\n",
    "    # plt.title('Variable Importance') \n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bcd88e-4b42-4fb2-8a79-e3cb1df6f1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
