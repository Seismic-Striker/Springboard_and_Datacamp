{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952218a6",
   "metadata": {},
   "source": [
    "# 1. Why generate features?\n",
    "- Hello and welcome to Feature Engineering for Machine Learning in Python. My name is Robert O’Callaghan and I am a Data Scientist.\n",
    "\n",
    "2. Feature Engineering\n",
    "Feature engineering is the act of taking raw data and extracting features from it that are suitable for tasks like machine learning. Most machine learning algorithms work with tabular data. When we talk about features, we are referring to the information stored in the columns of these tables. For example, if we were looking at information on houses, the features would be things like square foot, number of rooms, etc. This course is designed for data scientists who want to expand their knowledge of how to incorporate feature engineering into their data science workflow.\n",
    "\n",
    "3. Different types of data\n",
    "Most machine learning algorithms require their input data to be represented as a vector or a matrix, and many assume that the data is distributed normally. In the real world, more often than not you will receive data that is not in this format. You will also need to work with many different types of data, some data types you will often encounter are: continuous variables, categorical data, ordinal data, boolean values, and dates and times. Dealing with these is manageable, but requires a well thought out approach. Feature engineering is often overlooked in machine learning discussions, but any real-world practitioner will confirm that data manipulation and feature engineering is the most important aspect of the project.\n",
    "\n",
    "4. Course structure\n",
    "Over the span of this course, we will be addressing how to deal with many different types of data and how to convert them into a format that can be easily used for machine learning. In the first chapter, you will ingest and create basic features from tabular data. In the second chapter, you will learn how to deal with data that has missing values. You will then move on to transforming your data so that it conforms to statistical assumptions often necessary for machine learning models, and finally, you will convert free form text into tabular data so it can be used with machine learning models.\n",
    "\n",
    "5. Pandas\n",
    "Now lets jump straight in with some examples. During this course we will be leveraging the pandas package substantially as it is very useful when working with data in tabular form. It is a common practice to import pandas using the pd alias. You can use the read_csv() function to import a CSV file and use the head() method to quickly look at the first few rows of the DataFrame.\n",
    "\n",
    "6. Dataset\n",
    "For the first three chapters of this course, you will be working with a modified subset of the Stackoverflow survey response data. This dataset records the details and preferences of hundreds of users of the StackOverflow website.\n",
    "\n",
    "7. Column names\n",
    "To see the features used in this subset, you can use the DataFrame columns attribute to print the names of all the columns in the DataFrame.\n",
    "\n",
    "8. Column types\n",
    "To print the data type of each column, you can use the dtypes attribute. Here you can see three different data types - integers, floats and objects - in pandas objects are columns that contain strings.\n",
    "\n",
    "9. Selecting specific data types\n",
    "Knowing the types of each column can be very useful if you are performing analysis based on a subset of specific data types. To do this, you can use the select_dtypes() method and pass a list of relevant data types to the include argument. For example, if you want to select only the integer columns, call the select_dtypes() method on df and set the include argument to 'int'.\n",
    "\n",
    "10. Lets get going!\n",
    "Lets get right into it and start practicing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13cdf4",
   "metadata": {},
   "source": [
    "# Getting to know your data\n",
    "Pandas is one the most popular packages used to work with tabular data in Python. It is generally imported using the alias pd and can be used to load a CSV (or other delimited files) using read_csv().\n",
    "\n",
    "You will be working with a modified subset of the Stackoverflow survey response data in the first three chapters of this course. This dataset records the details, and preferences of thousands of users of the StackOverflow website.\n",
    "\n",
    "Instructions 3/4\n",
    "0 XP\n",
    "Print the data type of each column in so_survey_df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8131c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SurveyDate                                    FormalEducation  \\\n",
      "0  2/28/18 20:20           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
      "1  6/28/18 13:26           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
      "2    6/6/18 3:37           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
      "3    5/9/18 1:06  Some college/university study without earning ...   \n",
      "4  4/12/18 22:41           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
      "\n",
      "   ConvertedSalary Hobby       Country  StackOverflowJobsRecommend  \\\n",
      "0              NaN   Yes  South Africa                         NaN   \n",
      "1          70841.0   Yes       Sweeden                         7.0   \n",
      "2              NaN    No       Sweeden                         8.0   \n",
      "3          21426.0   Yes       Sweeden                         NaN   \n",
      "4          41671.0   Yes            UK                         8.0   \n",
      "\n",
      "      VersionControl  Age  Years Experience Gender   RawSalary  \n",
      "0                Git   21                13   Male         NaN  \n",
      "1     Git;Subversion   38                 9   Male   70,841.00  \n",
      "2                Git   45                11    NaN         NaN  \n",
      "3  Zip file back-ups   46                12   Male   21,426.00  \n",
      "4                Git   39                 7   Male  £41,671.00  \n",
      "SurveyDate                     object\n",
      "FormalEducation                object\n",
      "ConvertedSalary               float64\n",
      "Hobby                          object\n",
      "Country                        object\n",
      "StackOverflowJobsRecommend    float64\n",
      "VersionControl                 object\n",
      "Age                             int64\n",
      "Years Experience                int64\n",
      "Gender                         object\n",
      "RawSalary                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import so_survey_csv into so_survey_df\n",
    "so_survey_df = pd.read_csv('Combined_DS_v10.csv')\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "print(so_survey_df.head())\n",
    "\n",
    "# Print the data type of each column\n",
    "print(so_survey_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596e9fb",
   "metadata": {},
   "source": [
    "# Selecting specific data types\n",
    "Often a dataset will contain columns with several different data types (like the one you are working with). The majority of machine learning models require you to have a consistent data type across features. Similarly, most feature engineering techniques are applicable to only one type of data at a time. For these reasons among others, you will often want to be able to access just the columns of certain types when working with a DataFrame.\n",
    "\n",
    "The DataFrame (so_survey_df) from the previous exercise is available in your workspace.\n",
    "\n",
    "Instructions\n",
    "70 XP\n",
    "Create a subset of so_survey_df consisting of only the numeric (int and float) columns.\n",
    "Print the column names contained in so_survey_df_num.\n",
    "\n",
    "\n",
    "Show Answer (-70 XP)\n",
    "Hint\n",
    "To specify numeric columns you can pass include=['int', 'float'] as an argument to select_dtypes().\n",
    "Use the .columns attribute to print the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b12f656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ConvertedSalary', 'StackOverflowJobsRecommend', 'Age',\n",
      "       'Years Experience'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create subset of only the numeric columns\n",
    "so_numeric_df = so_survey_df.select_dtypes(include=['int','float'])\n",
    "\n",
    "# Print the column names contained in so_survey_df_num\n",
    "print(so_numeric_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfedd93",
   "metadata": {},
   "source": [
    "# 1. Dealing with Categorical Variables\n",
    "Categorical variables are used to represent groups that are qualitative in nature. Some examples are colors, such as blue, red, black etc. or country of birth, such as Ireland, England or USA. While these can easily be understood by a human, you will need to encode categorical features as numeric values to use them in your machine learning models.\n",
    "\n",
    "2. Encoding categorical features\n",
    "As an example, here is a table which consists of the country of residence of different respondents in the Stackoverflow survey. To get from qualitative inputs to quantitative features, one may naively think that assigning every category in a column a number would suffice, for example India could be 1, USA 2 etc. But these categories are unordered, so assigning this order may greatly penalize the effectiveness of your model. Thus, you cannot allocate arbitrary numbers to each category as that would imply some form of ordering in the categories.\n",
    "\n",
    "3. Encoding categorical features\n",
    "Instead, values can be encoded by creating additional binary features corresponding to whether each value was picked or not as shown in the table on the right. In doing so your model can leverage the information of what country is given, without inferring any order between the different options.\n",
    "\n",
    "4. Encoding categorical features\n",
    "There are two main approaches when representing categorical columns in this way, one hot encoding and dummy encoding. These are very similar and often confused. In fact, by default, pandas performs one-hot encoding when you use the get_dummies() function.\n",
    "\n",
    "5. One-hot encoding\n",
    "One-hot encoding converts n categories into n features as shown here. You can use the get_dummies() function to one-hot encode columns. The function takes a DataFrame and a list of categorical columns you want converted into one hot encoded columns, and returns an updated DataFrame with these columns included. Specifying a prefix with the prefix argument can improve readability like the letter C for country has been used here.\n",
    "\n",
    "6. Dummy encoding\n",
    "On the other hand, dummy encoding creates n-1 features for n categories, omitting the first category. Notice that this time there is no feature for France, the first category. In dummy encoding, the base value, France in this case, is encoded by the absence of all other countries as you can see on the last row here and its value is represented by the intercept. For dummy encoding, you can use the same get_dummies() function with an additional argument, drop_first set to True as shown here.\n",
    "\n",
    "7. One-hot vs. dummies\n",
    "Both these methods have different advantages. One-hot encoding generally creates much more explainable features, as each country will have its own weight that can be observed after training. But one must be aware that one hot encoding may create features that are entirely collinear due to the same information being represented multiple times.\n",
    "\n",
    "8. One-hot vs. dummies\n",
    "Take for example a simpler categorical column recording the sex of the survey takers. By recording a 1 for male the information of whether the person is female is already known when the male column is 0. This double representation can lead to instability in your models and dummy values would be more appropriate.\n",
    "\n",
    "9. Limiting your columns\n",
    "However, both one-hot encoding and dummy encoding may result in a huge number of columns being created if there are too many different categories in a column. In these cases, you may want to only create columns for the most common values. You can check the number of occurrences of different features in a column using the value_counts() method on a specific column.\n",
    "\n",
    "10. Limiting your columns\n",
    "Once you have your counts of occurrences, you can use it to limit what values you will include by first creating a mask of the values that occur less than n times. A mask is a list of booleans outlining which values in a column should be affected. First we find the categories that occur less than n times using the index attribute and wrap this inside the isin() method. After you create the mask, you can use it to replace these categories that occur less than n times with a value of your choice as shown here.\n",
    "\n",
    "11. Now you deal with categorical variables\n",
    "Lets put what has been learned into practice and work with some categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e9225",
   "metadata": {},
   "source": [
    "# One-hot encoding and dummy variables\n",
    "To use categorical variables in a machine learning model, you first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables. In this exercise, you will create both types of encoding, and compare the created column sets. We will continue using the same DataFrame from previous lesson loaded as so_survey_df and focusing on its Country column.\n",
    "\n",
    "Instructions 2/2\n",
    "35 XP\n",
    "One-hot encode the Country column, adding \"OH\" as a prefix for each column.\n",
    "\n",
    "Create dummy variables for the Country column, adding \"DM\" as a prefix for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc0bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby',\n",
      "       'StackOverflowJobsRecommend', 'VersionControl', 'Age',\n",
      "       'Years Experience', 'Gender', 'RawSalary', 'OH_France', 'OH_India',\n",
      "       'OH_Ireland', 'OH_Russia', 'OH_South Africa', 'OH_Spain', 'OH_Sweeden',\n",
      "       'OH_UK', 'OH_USA', 'OH_Ukraine'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convert the Country column to a one hot encoded Data Frame\n",
    "one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\n",
    "\n",
    "# Print the columns names\n",
    "print(one_hot_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb80b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby',\n",
      "       'StackOverflowJobsRecommend', 'VersionControl', 'Age',\n",
      "       'Years Experience', 'Gender', 'RawSalary', 'DM_India', 'DM_Ireland',\n",
      "       'DM_Russia', 'DM_South Africa', 'DM_Spain', 'DM_Sweeden', 'DM_UK',\n",
      "       'DM_USA', 'DM_Ukraine'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for the Country column\n",
    "dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\n",
    "\n",
    "# Print the columns names\n",
    "print(dummy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2a32a",
   "metadata": {},
   "source": [
    "# Dealing with uncommon categories\n",
    "Some features can have many different categories but a very uneven distribution of their occurrences. Take for example Data Science's favorite languages to code in, some common choices are Python, R, and Julia, but there can be individuals with bespoke choices, like FORTRAN, C etc. In these cases, you may not want to create a feature for each value, but only the more common occurrences.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Extract the Country column of so_survey_df as a series and assign it to countries.\n",
    "Find the counts of each category in the newly created countries series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25abe221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Africa    166\n",
      "USA             164\n",
      "Spain           134\n",
      "Sweeden         119\n",
      "France          115\n",
      "Russia           97\n",
      "UK               95\n",
      "India            95\n",
      "Other            14\n",
      "Name: Country, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derek\\AppData\\Local\\Temp/ipykernel_24264/1238127852.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  countries[mask] = 'Other'\n"
     ]
    }
   ],
   "source": [
    "# Create a series out of the Country column\n",
    "countries = so_survey_df['Country']\n",
    "\n",
    "# Get the counts of each category\n",
    "country_counts = countries.value_counts()\n",
    "\n",
    "# Create a mask for only categories that occur less than 10 times\n",
    "mask = countries.isin(country_counts[country_counts < 10].index)\n",
    "\n",
    "# Label all other categories as Other\n",
    "countries[mask] = 'Other'\n",
    "\n",
    "# Print the updated category counts\n",
    "print(countries.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef815e90",
   "metadata": {},
   "source": [
    "1. Numeric variables\n",
    "As mentioned in the previous lesson, most machine learning models will require your data to be in numeric format. However, even if your raw data is all numeric, there is still a lot you can do to improve your features.\n",
    "\n",
    "2. Types of numeric features\n",
    "Numeric features can be used to represent a huge array of different characteristics and measurements. Pretty much anything that can be quantitatively measured can be recorded as numeric data. For example, age, the price of an item, counts, and even spatial data such as coordinates. Depending on the use case, numeric features can be treated in several different ways. We will work through a few of the considerations and possible feature engineering steps to keep in mind when dealing with numeric data.\n",
    "\n",
    "3. Does size matter?\n",
    "One of the first questions you should ask when working with numeric features is whether the magnitude of the feature is its most important trait, or just its direction. For example, if you had a dataset of restaurant health and safety ratings containing the number of times a restaurant had major violations, you might care far more about whether the restaurant had any major violations at all (as you would rather not take any chances), over whether it was a repeat offender. Looking at this toy dataset containing restaurant IDs and the number of times they had major violations, we can see that some restaurants have no major violations but many have one or more. We will be creating a new binary column representing whether or not a restaurant committed any violation.\n",
    "\n",
    "4. Binarizing numeric variables\n",
    "Here we first create a new column Binary_Violation and set it to zero. Then, we use the dot loc notation to find all rows where Number_of_Violations is greater than zero and set the Binary_Violation column to 1.\n",
    "\n",
    "5. Binarizing numeric variables\n",
    "As you can see here, all rows where Number_of_Violations is equal to 0 are also zeros in Binary_Violation. However, for all rows where Number_of_Violations is greater than zero is 1 in Binary_Violation.\n",
    "\n",
    "6. Binning numeric variables\n",
    "An extension of this is perhaps you wish to group a numeric variable into more than two bins. This is often useful for variables such as age, wage brackets, etc where exact numbers are less relevant than the general magnitude of the value. Consider the same dataset of restaurant health and safety ratings containing the number of times a restaurant has had major violations. This time we will be creating three groups, Group 1, for restaurants with no offenses, Group 2 for restaurants with one or two offenses and group 3 for all restaurants with three or more offenses. Bins are created by using the pandas' cut() function. You can define the intervals using the bins argument as shown here, which in this case is a list of 4 values. You can also pass a list of labels like so.\n",
    "\n",
    "7. Binning numeric variables\n",
    "Note as we want to include 0 in the first bin, we must set the leftmost edge to lower than that, so all values between negative infinity and 0 are labeled as 1, all values equal to 1 or 2 are labeled as 2, and values greater than 2 are labeled as 3.\n",
    "\n",
    "8. Lets start practicing!\n",
    "Now you know how to binarize and bin numeric columns, it's time for you to put this into practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b369e3d",
   "metadata": {},
   "source": [
    "# Binarizing columns\n",
    "While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. In the so_survey_df data, you have a large number of survey respondents that are working voluntarily (without pay). You will create a new column titled Paid_Job indicating whether each person is paid (their salary is greater than zero).\n",
    "\n",
    "Instructions\n",
    "70 XP\n",
    "Create a new column called Paid_Job filled with zeros.\n",
    "Replace all the Paid_Job values with a 1 where the corresponding ConvertedSalary is greater than 0.\n",
    "\n",
    "\n",
    "Show Answer (-70 XP)\n",
    "Hint\n",
    "A new column can be created using df[column_name] = default_value.\n",
    "To set a value on a subset of rows, you can use df.loc[rows, column] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3bcfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Paid_Job  ConvertedSalary\n",
      "0         0              NaN\n",
      "1         1          70841.0\n",
      "2         0              NaN\n",
      "3         1          21426.0\n",
      "4         1          41671.0\n"
     ]
    }
   ],
   "source": [
    "# Create the Paid_Job column filled with zeros\n",
    "so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
    "so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print the first five rows of the columns\n",
    "print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910d086",
   "metadata": {},
   "source": [
    "# Binning values\n",
    "For many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages.\n",
    "\n",
    "Bins are created using pd.cut(df['column_name'], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "Bin the value of the ConvertedSalary column in so_survey_df into 5 equal bins, in a new column called equal_binned.\n",
    "\n",
    "\n",
    "Take Hint (-15 XP)\n",
    "2\n",
    "Bin the ConvertedSalary column using the boundaries in the list bins and label the bins using labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d540793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          equal_binned  ConvertedSalary\n",
      "0                  NaN              NaN\n",
      "1  (-2000.0, 400000.0]          70841.0\n",
      "2                  NaN              NaN\n",
      "3  (-2000.0, 400000.0]          21426.0\n",
      "4  (-2000.0, 400000.0]          41671.0\n"
     ]
    }
   ],
   "source": [
    "# Bin the continuous variable ConvertedSalary into 5 bins\n",
    "so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], 5)\n",
    "\n",
    "# Print the first 5 rows of the equal_binned column\n",
    "print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe51782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  boundary_binned  ConvertedSalary\n",
      "0             NaN              NaN\n",
      "1          Medium          70841.0\n",
      "2             NaN              NaN\n",
      "3             Low          21426.0\n",
      "4             Low          41671.0\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Specify the boundaries of the bins\n",
    "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# Bin labels\n",
    "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary using these boundaries\n",
    "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'], \n",
    "                                         bins, labels = labels)\n",
    "\n",
    "# Print the first 5 rows of the boundary_binned column\n",
    "print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8a58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
