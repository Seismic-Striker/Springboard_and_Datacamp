{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ada77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.DataFrame()\n",
    "data['column_name'] = np.empty(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94e1ea",
   "metadata": {},
   "source": [
    "# 1 imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3d040",
   "metadata": {},
   "source": [
    "numerical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a185d",
   "metadata": {},
   "source": [
    "categorical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5131d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts()\n",
    ".idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c345783",
   "metadata": {},
   "source": [
    "# 2 Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a3c6c",
   "metadata": {},
   "source": [
    "outlier detection with standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "\n",
    "#z-score can also be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d1d60",
   "metadata": {},
   "source": [
    "outlier determination with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69893521",
   "metadata": {},
   "source": [
    "An outlier Dilemma: Drop or Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96031080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8e365",
   "metadata": {},
   "source": [
    "# 3 Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935143a1",
   "metadata": {},
   "source": [
    "`#Numerical Binning Example\n",
    "Value      Bin       \n",
    "0-30   ->  Low       \n",
    "31-70  ->  Mid       \n",
    "71-100 ->  High\n",
    "#Categorical Binning Example\n",
    "Value      Bin       \n",
    "Spain  ->  Europe      \n",
    "Italy  ->  Europe       \n",
    "Chile  ->  South America\n",
    "Brazil ->  South America`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cad4e",
   "metadata": {},
   "source": [
    "#Numerical Binning Example\n",
    "**`data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])`**\n",
    "`   value   bin\n",
    "0      2   Low\n",
    "1     45   Mid\n",
    "2      7   Low\n",
    "3     85  High\n",
    "4     28   Low`\n",
    "`#Categorical Binning Example\n",
    "     Country\n",
    "0      Spain\n",
    "1      Chile\n",
    "2  Australia\n",
    "3      Italy\n",
    "4     Brazil`\n",
    "\n",
    "**`conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]`**\n",
    "\n",
    "**`choices = ['Europe', 'Europe', 'South America', 'South America']`**\n",
    "\n",
    "**`data['Continent'] = np.select(conditions, choices, default='Other')`**\n",
    "`     Country      Continent\n",
    "0      Spain         Europe\n",
    "1      Chile  South America\n",
    "2  Australia          Other\n",
    "3      Italy         Europe\n",
    "4     Brazil  South America`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81a10c",
   "metadata": {},
   "source": [
    "# 4 Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)\n",
    "   value  log(x+1)  log(x-min(x)+1)\n",
    "0      2   1.09861          3.25810\n",
    "1     45   3.82864          4.23411\n",
    "2    -23       nan          0.00000\n",
    "3     85   4.45435          4.69135\n",
    "4     28   3.36730          3.95124\n",
    "5      2   1.09861          3.25810\n",
    "6     35   3.58352          4.07754\n",
    "7    -12       nan          2.48491"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e119b",
   "metadata": {},
   "source": [
    "# 5 One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed79f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "data = data.join(encoded_columns).drop('column', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca15560",
   "metadata": {},
   "source": [
    "# 6.Grouping Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f674e",
   "metadata": {},
   "source": [
    "**Categorical Column Grouping**\n",
    "\n",
    "I suggest three different ways for aggregating categorical columns:\n",
    "\n",
    "The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.\n",
    "\n",
    "\n",
    "`data.groupby('id').agg(lambda x: x.value_counts().index[0])`\n",
    "\n",
    "\n",
    "Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative.\n",
    "\n",
    "\n",
    "Pivot table example: Sum of Visit Days grouped by Users\n",
    "\n",
    "`#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)`\n",
    "\n",
    "\n",
    "Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime. You can check the next section for the explanation of numerical column grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab0eb1",
   "metadata": {},
   "source": [
    "numerical columns grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdd6b3",
   "metadata": {},
   "source": [
    "# 7 feature split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cde3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.name\n",
    "0  Luther N. Gonzalez\n",
    "1    Charles M. Young\n",
    "2        Terry Lawson\n",
    "3       Kristen White\n",
    "4      Thomas Logsdon\n",
    "#Extracting first names\n",
    "data.name.str.split(\" \").map(lambda x: x[0])\n",
    "0     Luther\n",
    "1    Charles\n",
    "2      Terry\n",
    "3    Kristen\n",
    "4     Thomas\n",
    "#Extracting last names\n",
    "data.name.str.split(\" \").map(lambda x: x[-1])\n",
    "0    Gonzalez\n",
    "1       Young\n",
    "2      Lawson\n",
    "3       White\n",
    "4     Logsdon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc685181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#String extraction example\n",
    "data.title.head()\n",
    "0                      Toy Story (1995)\n",
    "1                        Jumanji (1995)\n",
    "2               Grumpier Old Men (1995)\n",
    "3              Waiting to Exhale (1995)\n",
    "4    Father of the Bride Part II (1995)\n",
    "data.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]\n",
    "0    1995\n",
    "1    1995\n",
    "2    1995\n",
    "3    1995\n",
    "4    1995"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80cecb",
   "metadata": {},
   "source": [
    "# 8 scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03525947",
   "metadata": {},
   "source": [
    "normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1907c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  normalized\n",
      "0      2    0.231481\n",
      "1     45    0.629630\n",
      "2    -23    0.000000\n",
      "3     85    1.000000\n",
      "4     28    0.472222\n",
      "5      2    0.231481\n",
      "6     35    0.537037\n",
      "7    -12    0.101852\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ceecf6",
   "metadata": {},
   "source": [
    "standardization\n",
    "\n",
    "standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.\n",
    "In the following formula of standardization, the mean is shown as μ and the standard deviation is shown as σ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5806a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  standardized\n",
      "0      2     -0.518878\n",
      "1     45      0.703684\n",
      "2    -23     -1.229670\n",
      "3     85      1.840952\n",
      "4     28      0.220346\n",
      "5      2     -0.518878\n",
      "6     35      0.419367\n",
      "7    -12     -0.916922\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07a4edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  year  month  passed_years  passed_months   day_name\n",
      "0 2017-01-01  2017      1             5             61     Sunday\n",
      "1 2008-12-04  2008     12            14            158   Thursday\n",
      "2 1988-09-11  1988      9            34            401     Sunday\n",
      "3 1999-08-25  1999      8            23            270  Wednesday\n",
      "4 1993-02-20  1993      2            29            348   Saturday\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = pd.DataFrame({'date':\n",
    "['01-01-2017',\n",
    "'04-12-2008',\n",
    "'11-09-1988',\n",
    "'25-08-1999',\n",
    "'20-02-1993',\n",
    "]})\n",
    "\n",
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86649da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
